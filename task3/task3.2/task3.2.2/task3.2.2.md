# task3.2.2的实验记录整合

> 目前我们已经完成了所有的必做题

## task3.2.2.1.1的试验记录

**产品推荐：Santander Product Recommendation:**

该项目数据量十分巨大，需要至少10G内存才能稳定读取并处理，
受限于设备配置，我们一开始尝试分片读取数据并处理，
但是这样做不仅增加了数据分析的难度，而且使得代码运行效率低下。

在阅读了网上相关思路并实际比较后，我们发现各个月的用户购买的产品关联性并不大，
因此我们最终只保留了去年六月的数据。

缩小数据量后，我们对数据做了一些处理，用众数、平均数、中位数填补了某些列的空值，
按省份分组，按各组中的中位数填补了renta这一列的值，因为我们认为renta在特征中的重要性会比较大，
而它显然会受到所在地区更大的影响。

为了减小所需算力，我们直接对非数值数据进行了数值处理，
并使用MLP对数据进行训练，在kaggle得分为0.014（排名1400/1800)
成绩并不是很理想。

在对特征进行权重微调，并用onehot编码代替部分特征原先的数值编码，
最终在kaggle的得分上升显著，达到了0.02100（排名1200/1800)
但距离最高分0.0314仍有很大的差距。

但是随后我们尝试了特征提取和模型构建上的各种微调，
Loss都没有再继续下降，在test集上的表现也没有变得更好。

我们推测得分不能继续提高的原因有一下三点：
1.是由于硬件限制我们只保留了六月的数据，忽略了其他月的数据可能包含的信息
2.我们第一次尝试在kaggle做classification，对特征的把握并不优秀，有待提高
3.网上主流使用的都是xgboost进行分类，在分类问题上MLP可能并不如传统算法优秀

我们接下来将进行xgboost的学习，并在接下来的3.2.2项目中使用xgboost代替MLP模型，
以期获得更好的分类结果。

## task3.2.2.2.1

**商品销售预测：Predict Future Sales: **

该题是基于前34个月的销量数据，对下个月的数据进行预测，
数据是由日常销售数据组成的时间序列数据集，提供了包括商店，商品，价格，日销量等连续34个月内的数据评价指标为RMSE吸取了之前使用MLP，结果并不理想的教训
我们在这次的任务中选取xgboost作为模型由于预测的是月销量，我们首先将数据按月聚合，并增加月份的数据并加入了商品的类别，这些简单的特征最终的rmse数值超过了5，表现得非常糟糕

考虑到上面简单的特征并没有提取出太多有用的信息，由于该任务是基于时间序列数据，我们又添加了上个月商店的总销量，上个月商品的平均售价，上个月商品的总销量这几个特征
此时rmse显著减小了，但仍然在2左右在思考和尝试之后，我们发现，数据并没有包含所有的商品组合，而没有出现的商品组合，他们的月销量应该为0，补充完所有商品组合的数据，并将销量置零后rmse再次显著降低，达到1

最终考虑到项目说明中，所有的月销量在[0,20]的区间上，我们将所有训练数据中月销量大于20的赋值成20并将最终测试数据在模型上的结果约束在这一区间内，最终在kaggle上的得分提升至0.09 排名进入了前30%

## task3.2.2.3.1

由于数据过大，我们选择先1000000个数据进行训练

输出后可以看到数据中没有缺失值我们先尝试只用数值数字训练
模型选用xgboost

单单如此我们竟然就在kaggle上取得了0.45503分的好成绩，虽然排名只有1411/1602，但我们的分数与中上分段的差距并不大，而我们还有很大的提升空间，
接下来我们可以优化的点主要有4个：
- 1.对数值数据进行归一化等处理 
- 2.对非数值的数据用one-hot编码等方式数值化，加入训练模型中 
- 3.使用数据采样提取数据，而不是简单的使用前100000行数据 
- 4.调整模型参数，或者考虑使用LightGBM代替xgboost模型

首先是数据采样
train.csv是按照时间排序的，只读取前100w条记录，会导致采样偏差
我们将用户均匀分组，从每个分组去取合适数量的实例，以保证采取的样本尽可能对总体有代表性

我们对label按比例取样，使得采样后click=1的数据占比不变我们按0.05的大小比例获取样本，减小数据集，方便后序操作

在采样后的数据上再尝试一次

在使用采样后的数据后，我们交叉验证的auc甚至降低了，表现不如从前
这可能是原来连续的数据间有很多的共性，我们推测这并不代表我们预测的准度下降了这一猜想是正确的，事实上再提交后，我们在kaggle上的分数整整提高了0.03，已经达到了0.42006 排名1217/1602
可见取样的重要性观察非数值特征id和site类的unique值非常之大，我们认为这缺少分类的意义

由于我们考虑使用one-hot编码进行数值化两个category列不仅unique值很小，而且也和click有很大的现实联系
所以我们暂且只考虑这两列使用one-hot编码增加特征后我们的得分再次有了明显的提高
在kaggle的得分提升至0.40069排名1073/1602微调了xgb树高度限制 5->6 之后我们再次提交
没有啥变化看来没啥用，我们再改回去
接下来是一些归一化处理归一化之后得分反而下降了
这并不令人惊讶因为决策树并不是很需要归一化
往往会带来负收益

因此我们最终的得分是one-hot编码后未进行归一化的得分
即0.40069