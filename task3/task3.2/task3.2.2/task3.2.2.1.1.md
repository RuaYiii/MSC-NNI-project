# task3.2.2.1.1的试验记录

该项目数据量十分巨大，需要至少10G内存才能稳定读取并处理，
受限于设备配置，我们一开始尝试分片读取数据并处理，
但是这样做不仅增加了数据分析的难度，而且使得代码运行效率低下。

在阅读了网上相关思路并实际比较后，我们发现各个月的用户购买的产品关联性并不大，
因此我们最终只保留了去年六月的数据。

缩小数据量后，我们对数据做了一些处理，用众数、平均数、中位数填补了某些列的空值，
按省份分组，按各组中的中位数填补了renta这一列的值，因为我们认为renta在特征中的重要性会比较大，
而它显然会受到所在地区更大的影响。

为了减小所需算力，我们直接对非数值数据进行了数值处理，
并使用MLP对数据进行训练，在kaggle得分为0.014（排名1400/1800)
成绩并不是很理想。

在对特征进行权重微调，并用onehot编码代替部分特征原先的数值编码，
最终在kaggle的得分上升显著，达到了0.02100（排名1200/1800)
但距离最高分0.0314仍有很大的差距。

但是随后我们尝试了特征提取和模型构建上的各种微调，
Loss都没有再继续下降，在test集上的表现也没有变得更好。

我们推测得分不能继续提高的原因有一下三点：
1.是由于硬件限制我们只保留了六月的数据，忽略了其他月的数据可能包含的信息
2.我们第一次尝试在kaggle做classification，对特征的把握并不优秀，有待提高
3.网上主流使用的都是xgboost进行分类，在分类问题上MLP可能并不如传统算法优秀

我们接下来将进行xgboost的学习，并在接下来的3.2.2项目中使用xgboost代替MLP模型，
以期获得更好的分类结果。

